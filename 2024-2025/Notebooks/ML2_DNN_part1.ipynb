{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">AI-ML @ ENSPIMA / v1.2 september 2024 / Jean-Luc CHARLES (Jean-Luc.charles@mailo.com) / CC BY-SA 4.0 /</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:brown;font-family:arial;font-size:26pt;font-weight:bold;text-align:center\"> \n",
    "Machine learning with Python tensorflow2/keras modules</div><br>\n",
    "<hr>\n",
    "<div style=\"color:blue;font-family:arial;font-size:22pt;font-weight:bold;text-align:center\"> \n",
    "Training a Dense Neural Network to classify handwritten digits<br><br>\n",
    "DNN-Part-1 : Build and train a Dense Neural Network</div>\n",
    "<hr>\n",
    "Expected duration : 120 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:12pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for your Python projects: a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).</span></div>\n",
    "\n",
    "All the notebooks must be loaded in a `jupyter notebook` or `jupyter lab` launched within the <b><span style=\"color: rgb(200, 151, 102);\" >pyml</span></b> PVE specially created for the session.<br>\n",
    "They should be worked in this order:\n",
    "- `ML1_MNIST.ipynb`: check that the <b><span style=\"color: rgb(200, 151, 102);\">pyml</span></b> PVE is fuly operationnal, load and use the data from the MNIST database (images and labels).\n",
    "- `ML2_DNN_part1.ipynb`: build a Dense Neural Network (DNN), train it with data from the MNIST and evaluate its performance.\n",
    "- `ML2_DNN_part2.ipynb`: reload a previously trained DNN and evaluate its performance with the MNIST test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted learning objectives\n",
    "Know how to:\n",
    "- build a dense neural network with the Python modules **tensorflow** and **keras**.\n",
    "- train a dense network with data from the MNIST bank.\n",
    "- display the training performance curves.\n",
    "- save the structure and the weights of the trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Verify importing Python modules\n",
    "The **keras** module which allows high-level manipulation of **tensorflow** objects is integrated in the **tensorflow** (tf) module since version 2. <br>\n",
    "The **tf.keras** module documentation to consult is here: https://www.tensorflow.org/api_docs/python/tf/keras.\n",
    "\n",
    "Importing the `tensorflow` module in the cell below may generate some warning messages...<br>\n",
    "if errors appear they must be corrected, possibly by recreating your PVE <b><span style=\"color: rgb(200, 51, 102);\">pyml</span></b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, cv2\n",
    "\n",
    "# Delete the (numerous) warning messages from the **tensorflow** module:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# specific modules:\n",
    "from utils.tools import scan_dir, plot_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"numpy     : {np.__version__}\")\n",
    "print(f\"OpenCV    : {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding matplotlib plots in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense network structure to build\n",
    "In this step you will build a **dense network**, with:\n",
    "- an **input layer** of 784 values in the range [0 ; 1.]<br>\n",
    "(the pixels of the MNIST 28 $\\times$ 28 images put in the form of a vector of 784 normalized `float` numbers),\n",
    "- a **hidden layer** of 784 neurons using the `relu` activation function,\n",
    "- an **output layer** with 10 neurons, for the classification of images into 10 classes associated with the digits {0,1,2...9}, using the `softmax` activation function adapted to classification problems .\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic; font-size:12px;\">\n",
    "    <img src=\"img/ReseauChiffres-2_transp.png\" alt=\"archiNetwork.png\" style=\"width:900px;\"><br>\n",
    "    [image credit: JLC]\n",
    "</p>\n",
    "\n",
    "Remarks :\n",
    "- Each neuron of the first hidden layer receives 785 inputs: the 784 values $x_i$ of the pixels of the image plus the bias.\n",
    "- $\\leadsto$ There are therefore 785 unknowns for each neuron: the 784 weights $w_i$ assigned to each input $x_i$, plus the weight $b$ assigned to input $-1$.\n",
    "- $\\leadsto$ there are therefore 785 $\\times$ 784 unknowns for the hidden layer and 785 $\\times$ 10 unknowns for the output layer: i.e. a total of 623290 unknowns whose value must be optimized by the algorithm d network learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Work to do\n",
    "### 1 - Load MNIST images and define important parameters\n",
    "### 2 - Pre-process MNIST images and labels\n",
    "### 3 - Build the dense Neural Network \n",
    "### 4 - Save the struture & weights of the initial Network\n",
    "### 5 - A first \"naive\" Network training \n",
    "### 6 - Train the Network while measuring its performance at each *epoch*\n",
    "### 7 - Train the Network while measuring its performance at each *epoch* and managing the *over-fit*\n",
    "### 8 - Save the struture & weights of the trained Network.<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load MNIST images and define important parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work of loading MNIST images has already been seen in the *notebook* `ML1_MNIST.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(im_train, lab_train), (im_test, lab_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(\"im_train -> shape:\", im_train.shape, \", dtype:\", im_train.dtype,)\n",
    "print(\"im_test  -> shape:\", im_test.shape,  \", dtype:\", im_test.dtype,)\n",
    "print(\"lab_train-> shape:\", lab_train.shape,  \", dtype:\", lab_train.dtype)\n",
    "print(\"lab_test -> shape:\", lab_test.shape,  \", dtype:\", lab_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define important parameters\n",
    "\n",
    "To avoid \"hard writing\" the **number of training and test images**, the **dimension** of the images and the **number of classes** to recognize, these parameters are retrieved from existing object attributes:\n",
    "- the `shape` attribute of the `im_train` and `im_test` tables allows to extract the number of training and test images,\n",
    "- the `size` attribute of the first training (or test) image gives the number of pixels of the images (784),<br>\n",
    "- the transformation of the `lab_test` array into a Python `set` (a set) gives the set of labels to recognize, whose size is the number of classes.\n",
    "\n",
    "complete the cell below accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{nb_im_train} traing images and {nb_im_test} test images\")\n",
    "print(f\"{nb_pixel} pixels in each image\")\n",
    "print(f\"{nb_classe} classes to recognize (the digits from 0 to 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Pre-process MNIST images and labels\n",
    "\n",
    "Two treatments must be applied to the data from the MNIST database:\n",
    "- on the images: transform the matrices of  28$\\,\\times\\,$28 pixels (`uint8`integers) into **normalized** vectors $(V_i)_{i=0..783}$ of 784 real values $V_i$ with $ 0 \\leqslant V_i \\leqslant 1$;\n",
    "- on labels: transform scalar numbers into *one-hot* vectors.\n",
    "\n",
    "### Transform input matrices into normalized vectors\n",
    "\n",
    "Define the arrays `x_train` and `x_test` containing the matrices of the arrays `im_train` and `im_test` *flattened* as normalized vectors (values between 0 and 1).<br>\n",
    "*tips*:\n",
    "- use the `reshape` method of the *ndarray* class of *numpy* and the `nb_im_train`, `nb_im_test` and `nb_pixel` parameters previously defined,\n",
    "- normalization can be handled by dividing arrays by their max value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensions of the `x_train` and `x_test` arrays as well as their *min* and *max* values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *one-hot* encoding of labels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consult the documentation of the `to_categorical` function on the page [tf.keras.utils.to_categorical](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical) and deduce how to define the `y_train` and `y_test` arrays containing the *hot-one* encoded version of the `lab_train` and `lab_test` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually check the first 10 values of the `lab_train` and `y_train` arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Build the Dense Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a dense **sequential** neural network in Python **5 lines** using the **keras** module.\n",
    "\n",
    "Build the network incrementally in the cell below, following the proposed approach (look for the `add` method in the page [guide/keras/sequential_model](https://www.tensorflow.org/guide/keras/sequential_model) if necessary) :\n",
    "- 1/ Create the object `model` instance of the class `Sequential` (cf [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)).\n",
    "- 2/ With the `add` method of the `model` object add:\n",
    "    - the input layer `Input(shape=<number of neurons>)` (cf [tf.keras.layers.Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) )<br>\n",
    "    Use the `nb_pixel` parameter to specify the value of the `shape` parameter (which must be a `tuple`)...<br>\n",
    "    - the intermediate dense layer (cf [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)): `Dense(<number of neurons>, activation='relu')` (cf [tf.keras.activation.relu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu))\n",
    "    - the output dense layer: `Dense(<number of neurons>, activation='softmax')` (cf [tf.keras.activation.softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax)).<br>\n",
    "Use the `nb_pixel` and `nb_classe` parameters to indicate the number of neurons and the number of classes without 'writing them hard'...\n",
    "- 3/ Once built, the network must be compiled (in the sense of tensorflow) with the `compile` method and the arguments:\n",
    "    - `loss='categorical_crossentropy'`: choice of the error function (cf [tf.keras.categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy))\n",
    "    - `optimizer='adam'`: choice of Adam optimizer (see page [tf.keras.optimizers.Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) )\n",
    "    - `metrics=['accuracy']` to obtain training statistics to draw performance curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# the 5 lines to build the neural network:\n",
    "model = Sequential()\n",
    "...\n",
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: By using the `name` argument in the `Input` and `Dense` constructors, one can give custom names to the layers, which will appear in the `summary` and `plot_model` outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `summary` method of the `model` object, display the description of the model and check the dimensions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there some `None` in the \"Output Shape\" column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: Because the number of images that will be used for training in not given at the DNN construction stage but later, et the training stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the total number of parameters with a simple formula..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_model` function draws the structure of the network (see the page [tf.keras.utils.plot_model](https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model)). <br>\n",
    "Plot the model structure by adding the `show_shapes=True` option to the `plot_model` call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Save the structure & weights of the initial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the weights of the initial untrained network (random values) and its structure with the `model.save` method. <br>\n",
    "This will be useful later to re-create the network to its initial state if we want to compare difrent trainings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check wether the 'model' directory exist (create it if needed):\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# define a uniq key:\n",
    "key = 'dense1_init'\n",
    "\n",
    "# define the path where to store the network data:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# savue the structure and the weights of the current neural network:\n",
    "model.save(path)\n",
    "\n",
    "# display the tree beginning at f'./models/{key}':\n",
    "tree = scan_dir(f\"./models/{key}\")\n",
    "print(f'\\nFiles written:\\n{tree}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the `save` method uses the `key` part of its argument to prefix the created file names.<br>\n",
    "$\\leadsto$ When loading the DNN structure & weights later with the `tf.keras.models.load_model` method, we just use the same key to retrieve the relevant files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - A first \"naive\" Network training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, consult the documentation of the `fit` method on the page [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "Complete the cell below to train the DNN with the `fit` method of the `model` object using the arguments:\n",
    "- `x_train`: the 60000 flattened and normalized images\n",
    "- `y_train`: the 60000 *one-hot* encoded labels.\n",
    "- `epochs=15`: repeat the training 15 times.\n",
    "- `batch_size=128`: split the input data set (the 60000 images) into \"batches\" of size `batch_size` (here: batches of 128 images).<br>\n",
    "Updating network weights is done after each batch of `batch_size` images.<br>\n",
    "The value of `batch_size` (by default: 32) is a parameter that influences the quality of the training but also its memory footprint: you can later try different values (64, 128, 256 ...) and observe how the quality of the training evolves).\n",
    "\n",
    "Name `hist` the data returned by the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the key to reload the initial state & structure of the network:\n",
    "key = 'dense1_init'\n",
    "\n",
    "# define the path to be used:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# load the network initial structure and weights:\n",
    "model = tf.keras.models.load_model(path) \n",
    "\n",
    "# Deterministic tensorflow training: \n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# see https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "# train the DNN:\n",
    "hist = model.fit(  ...  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Can you explain why there are 469 updates of the DNN weights per epoch ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if we divide the datat set size:60000 by the batch_szie:128, we get 468.75\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hist` object returned by the `fit` method has a `history` attribute of type `dict` whose keys `'loss'` and `'accuracy'` are associated with the corresponding values at each _epoch_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the `loss` and `accuracy` curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_loss_accuracy` function of the `utils.tools` module (found in the notebook directory) plots the \"Model accuracy\" and \"Model loss\" curves with the data stored in `hist`.<br> Import and use the `plot_loss_accuracy` function to plot these curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Train the Network while measuring its performance at each *epoch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better indicator of the quality of the training, you can test at the end of each `epoch` the precision of the inferences of the trained network using the test data: just pass the `validation_data` argument to the `fit` method, assigning it the test data tuple `(x_test, y_test)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the key to reload the initial state & structure of the network:\n",
    "key = 'dense1_init'\n",
    "\n",
    "# define the path to be used:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# load the network initial structure and weights:\n",
    "model = tf.keras.models.load_model(path) \n",
    "\n",
    "# Deterministic tensorflow training: \n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# see https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "hist = model.fit(   ...   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `hist.history` dictionary has also the new keys `val_loss` and `val_accuracy` calculated with the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot again the curves with the `plot_loss_accuracy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the precision calculated with the test data tends towards a limit close to 98.3 %. You might think that increasing the value of `epochs` would improve the precision of the network... but you run the risk of over-training the network (*over-fit*)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Train the Network while measuring its performance at each *epoch* <br>and managing the *over-fit*\n",
    "\n",
    "The `Keras` module offers tools to automatically stop the training by monitoring for example the growth of precision (accuracy) from one `epoch` to another.<br>\n",
    "You define the parameters of the `EarlyStopping` (cf [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)) *callback* and pass it to the method `fit` via the `callbacks` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# define the parameters of the `EarlyStopping`:\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_accuracy',  # the parameter to monitor\n",
    "                  patience=2,              # accept that the parameter decreases only twice\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# define the key to reload the initial state & structure of the network:\n",
    "key = 'dense1_init'\n",
    "\n",
    "# define the path to be used:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# load the network initial structure and weights:\n",
    "model = tf.keras.models.load_model(path) \n",
    "\n",
    "# Deterministic tensorflow training: \n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# see https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of monitoring the decrease of `val_accuracy` you can also monitor the increase of `val_loss`, which may be a prefeerd strategy (can you guess why ?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# define the parameters of the `EarlyStopping`:\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_loss',  # the parameter to monitor\n",
    "                  patience=2,          # accept that 'val_loss' increases twice\n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# define the key to reload the initial state & structure of the network:\n",
    "key = 'dense1_init'\n",
    "\n",
    "# define the path to be used:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# load the network initial structure and weights:\n",
    "model = tf.keras.models.load_model(path) \n",
    "\n",
    "# Deterministic tensorflow training: \n",
    "# set the seed of the random generators inolved by tensorflow:\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "# see https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test),\n",
    "                 epochs=15, \n",
    "                 batch_size=128, \n",
    "                 callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8 - Save the structure & weights of the trained Network\n",
    "\n",
    "The `save` method of the `Sequential` class saves **the structure** and the **weights** of the trained DNN.<br>\n",
    "You can use later the `tf.keras.models.load_model` function to recreate the network and reload its trained weights to exploit it in operational situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check wether the 'model' directory exist (create it if needed):\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# define a uniq key:\n",
    "key = 'dense1_trained'\n",
    "\n",
    "# define the path where to store the network data:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# savue the structure and the weights of the current neural network:\n",
    "model.save(path)\n",
    "\n",
    "# display the tree beginning at f'./models/{key}':\n",
    "tree = scan_dir(f\"./models/{key}\")\n",
    "print(f'\\nFiles written:\\n{tree}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Further work:\n",
    "You can now load the `ML2_DNN_part2_en.ipynb` notebook to learn how to exploit the DNN you have just rained.\n",
    "\n",
    "## Other interesting resources... videos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/trWrEWfhTVg\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/aircAruvnKk\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/IHZwWFHWa-w\"width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://www.youtube.com/embed/Ilg3gGewQ5U\" width=\"400\" height=\"300\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
