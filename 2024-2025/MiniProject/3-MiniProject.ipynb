{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d562f1bc",
   "metadata": {},
   "source": [
    "<span style=\"font-size:10pt\">AI-ML @ ENSPIMA / v1.3 september 2024 / Jean-Luc CHARLES (Jean-Luc.charles@mailo.com) / CC BY-SA 4.0 /</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573c2ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"color:brown;font-family:arial;font-size:26pt;font-weight:bold;text-align:center\"> \n",
    "Machine Learning $-$ MiniProject</div><br>\n",
    "<hr>\n",
    "<div style=\"color:blue;font-family:arial;font-size:22pt;font-weight:bold;text-align:center\">\n",
    "Training a neural network to diagnose bearing faults<br><br>\n",
    "Part 3/3: The mini project.</div>\n",
    "<hr>\n",
    "\n",
    "Expected duration : 60 minutes\n",
    "\n",
    "## Part-3 targeted learning objectives\n",
    "- Know how to train/operate a DNN to diagnose bearing faults using a labeled temporal dataset.\n",
    "- The mini peoject: know how to train/operate a DNN to diagnose bearing faults using the spectral transform of a temporal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7ca95",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<span style=\"color:brown;font-family:arial;font-size:12pt\"> \n",
    "It is important to use a <span style=\"font-weight:bold;\">Python Virtual Environment</span> (PVE) for your Python projects: a PVE makes it possible to control for each project the versions of the Python interpreter and the \"sensitive\" modules (like tensorflow).\n",
    "    \n",
    "All the notebooks must be loaded in a `jupyter notebook` or `jupyter lab` launched within the <b><span style=\"color: rgb(100, 151, 202);\" >pyml</span></b> PVE specially created for the session.    \n",
    "</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# Delete the (numerous) warning messages from the **tensorflow** module:\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# specific modules:\n",
    "from utils.tools import scan_dir, plot_loss_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python    : {sys.version.split()[0]}\")\n",
    "print(f\"tensorflow: {tf.__version__} incluant keras {keras.__version__}\")\n",
    "print(f\"numpy     : {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f4765f",
   "metadata": {},
   "source": [
    "Reminder: The bearing data set was obtained under the experimental conditions\n",
    "- under normal condition (N)\n",
    "- with outer race fault (OF)\n",
    "- with inner race fault (IF)\n",
    "- with roller fault (OF).\n",
    "\n",
    "|class label|Fault type|Fault diameter|\n",
    "|:---------:|:--------:|-------------:|\n",
    "| 1         | N        | 0            |\n",
    "| 2         | RF       | 0.18         |\n",
    "| 3         | RF       | 0.36         |\n",
    "| 4         | RF       | 0.54         |\n",
    "| 5         | IF       | 0.36         |\n",
    "| 6         | IF       | 0.36         |\n",
    "| 7         | IF       | 0.54         |\n",
    "| 8         | OF       | 0.18         |\n",
    "| 9         | OF       | 0.36         |\n",
    "| 10        | OF       | 0.54         |\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625651cc",
   "metadata": {},
   "source": [
    "# 4 $-$ A first try to train the neural network with the temporal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9696c6",
   "metadata": {},
   "source": [
    "## 4.1 $-$ Load the *CWRU* data and define some useful objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb337c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load('CWRU_dadaset.npz')\n",
    "A, B, C = npzfile.values()\n",
    "full_dataset = (A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bc10b",
   "metadata": {},
   "source": [
    "Let's have a look at the arrays shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e465854",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape, B.shape, C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8cda32",
   "metadata": {},
   "source": [
    "As you can see, the dimensions of the arrays are : (#health_conditions, #samples, #data_points).\n",
    "\n",
    "Let's define:\n",
    "- `H` $\\leadsto$ the number of health conditions\n",
    "- `S` $\\leadsto$ the number of samples per health contion\n",
    "- `N` $\\leadsto$ the number of data points per sample\n",
    "- `L` $\\leadsto$ the number of load cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, S, N = A.shape           # we can use, A,B or C : they have the same shape\n",
    "L = len(full_dataset)\n",
    "\n",
    "print(f\"L={L} load cases of the motor,\")\n",
    "print(f\"H={H} health conditions,\")\n",
    "print(f\"S={S} samples per health conditions,\")\n",
    "print(f\"=> giving LxHxS={L*H*S} samples of N={N} data points per sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c35f01",
   "metadata": {},
   "source": [
    "Let's define the list of the labels for the health conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the list of the health condition labels:\n",
    "health_cond = ['N']\n",
    "for def_type in 'RF', 'IF', 'OF':\n",
    "    for size in '18', '36', '54':\n",
    "        health_cond.append(f\"{def_type}.{size}\")\n",
    "print(f\"list of {len(health_cond)} health conditions:\", health_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9157e",
   "metadata": {},
   "source": [
    "## 4.2 $-$ Prepare the labeled dataset for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d020f72",
   "metadata": {},
   "source": [
    "We have two actions to do:\n",
    "- merge the 200 samples for each of the 10 health conditions and each of the 3 load cases into a single array of 200 $\\times$ 10 $\\times$ 3 = 6000 samples,\n",
    "- build the array of the corresponding 6000 labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627a4f08",
   "metadata": {},
   "source": [
    "First we define the arrays with the right shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a42c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.ndarray((L*H*S, N), dtype='float')  # the array of the temporal samples\n",
    "y_full = np.ndarray((L*H*S,), dtype='uint8')    # the array of the labels of the defects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b131ea",
   "metadata": {},
   "source": [
    "Let's verify the shape of the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9bd83e",
   "metadata": {},
   "source": [
    "Then we fill `x_full` with samples and `y_full` with corresponding labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3030ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for data in (A, B, C):       # browse the 3 load cases\n",
    "    for h in range(H):       # browse the 10 health conditions (gives the label)\n",
    "        for s in range(S):   # browse the 200 samples\n",
    "            x_full[i] = data[h, s]\n",
    "            y_full[i] = h    # the label is given by the health condition loop variable\n",
    "            i += 1                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad859a",
   "metadata": {},
   "source": [
    "To verify, let's plot the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1021398",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63e5b4",
   "metadata": {},
   "source": [
    "### Normalization of the temporal samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4349f7",
   "metadata": {},
   "source": [
    "We normalize the samples of `x_full` by dividing each one by the max of its absolute values.<br>\n",
    "There are two ways to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83e5f0b",
   "metadata": {},
   "source": [
    "a/ Version with an **explicit loop** to browse through all the samples of `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_full)):\n",
    "    x_full[i] = x_full[i]/np.abs(x_full[i]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69396c",
   "metadata": {},
   "source": [
    "Let's take a look now at the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29294092",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085ebf7",
   "metadata": {},
   "source": [
    "b/ the numpy **vectorized** style (more efficient):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556bff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = x_full/np.abs(x_full).max(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aeb063",
   "metadata": {},
   "source": [
    "Let's take a look now at the sample of rank 10 in `x_full`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0771f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e855512",
   "metadata": {},
   "source": [
    "$\\leadsto$ the values of `x_full` are now all in the range [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220b567",
   "metadata": {},
   "source": [
    "## 4.3 Split the full dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7de80",
   "metadata": {},
   "source": [
    "Thanks to the `train_test_split` function of the `sklearn.model_selection` module we can split the `x_full` and `y_full` ndarrays into a _train_ dataset (for the training) and a _test_ dataset (for testing).<br>\n",
    "Samples and labels are randomly selected but respecting the proportion of each of the 10 classes in the original dataset (this is the interest of the `stratify` argument of the `train_test_split` function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f3801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, lab_train, lab_test = train_test_split(x_full, y_full, \n",
    "                                                        stratify=y_full,      # use y_full to evenly distribute all classes \n",
    "                                                                              # in the train and test dadasets\n",
    "                                                        test_size=0.33,        # 20 % test, 80% train \n",
    "                                                        random_state=SEED, \n",
    "                                                        shuffle=True)         # shuffe randomly the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd992e",
   "metadata": {},
   "source": [
    "## 4.4 Transform labels to *one-hot* format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60832c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding' des labels :\n",
    "y_train = to_categorical(lab_train)\n",
    "y_test  = to_categorical(lab_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db361e19",
   "metadata": {},
   "source": [
    "Just a recap of the shapes of the arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, lab_train.shape, lab_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c171ba",
   "metadata": {},
   "source": [
    "## 4.6 $-$ Build the Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a4a41",
   "metadata": {},
   "source": [
    "You will build a dense neural network with this structure:\n",
    "\n",
    "     Input layer : N inputs\n",
    "     Hidden layer 'h1' : N neurones, activation fucntion: relu                                                    \n",
    "     Hidden layer 'h2' : 600  neurones, activation fucntion: relu                          \n",
    "     Hidden layer 'h3' : 200  neurones, activation fucntion: relu                         \n",
    "     Hidden layer 'h4' : 100  neurones, activation fucntion: relu\n",
    "     Output layer      :  H   neurones, activation fucntion: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0df6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Build the neural network layer by layer:\n",
    "model = Sequential()\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184501c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50f9ac",
   "metadata": {},
   "source": [
    "Let's save the structure and the initial weights of the network if we want to rebuild later the network to its initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae549ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check wether the 'model' directory exist (create it if needed):\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# define a uniq key:\n",
    "key = 'CWRU_temporal_init'\n",
    "\n",
    "# define the path where to store the network data:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# savue the structure and the weights of the current neural network:\n",
    "model.save(path)\n",
    "\n",
    "# display the tree beginning at f'./models/{key}':\n",
    "tree = scan_dir(f\"./models/{key}\")\n",
    "print(f'\\nFiles written:\\n{tree}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18360f0a",
   "metadata": {},
   "source": [
    "Now we train the neural network with `x_train` & `y_train` as the labeled dataset and `x_test` & `y_test` as the validation labeled dataset to use at the end of each epoch to mesure the network performance.<br>\n",
    "To avoid any *over-fitting* we use the `EarlyStopping` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cc87b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callbacks_list = [ \n",
    "    EarlyStopping(monitor='val_loss',    \n",
    "                  patience=2,       \n",
    "                  restore_best_weights=True,\n",
    "                  verbose=1)\n",
    "]\n",
    "\n",
    "# in case we execute a training several times, we re-build the network \n",
    "# to its initial state if we want to compare the workouts...\n",
    "\n",
    "# define the key to reload the initial state & structure of the network:\n",
    "key = 'CWRU_temporal_init'\n",
    "# define the path to be used:\n",
    "model_path = os.path.join('models', key)\n",
    "# load the network structure & initial weights:\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Deterministic tensorflow training: \n",
    "# see https://blog.tensorflow.org/2022/05/whats-new-in-tensorflow-29.html\n",
    "tf.keras.utils.set_random_seed(SEED)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism() \n",
    "\n",
    "# train the DNN:\n",
    "hist = model.fit(x_train, y_train,\n",
    "                 validation_data=(x_test, y_test), \n",
    "                 epochs=50, \n",
    "                 batch_size=64,                     # number of samples in the batch\n",
    "                 callbacks = callbacks_list)\n",
    "\n",
    "from utils.tools import plot_loss_accuracy\n",
    "plot_loss_accuracy(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91734c97",
   "metadata": {},
   "source": [
    "Now we compute the trained network predictions for the test datatset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bb90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(x_test)          # restults is an array of probabilities vectors\n",
    "inferences = results.argmax(axis=-1)     # extract the highest probablities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abea49",
   "metadata": {},
   "source": [
    "And we can plot the **confusion matrix** to  see if the networkd is well trained or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "axis = plt.axes()\n",
    "ConfusionMatrixDisplay.from_predictions(lab_test, inferences, \n",
    "                                        ax=axis,\n",
    "                                        display_labels=health_cond, \n",
    "                                        xticks_rotation='vertical',\n",
    "                                        colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e74b43",
   "metadata": {},
   "source": [
    "Not all the bearing defaults are classified with a good score...<br>\n",
    "$\\leadsto$ the next step is to try to train the network with the spectral datasets computed from the temporal samples to see if it's better ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02f60f",
   "metadata": {},
   "source": [
    "# 5 $-$ The mini project: Train the neural network with the spectrum data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072d15c",
   "metadata": {},
   "source": [
    "## 5.1 $-$ Compute the spectral datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c375cc",
   "metadata": {},
   "source": [
    "See **3.1 − Compute the spectral datasets** in the notebook *2-process_CWRU_data.ipynb* to help you to do the work...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "H, S, N = A.shape\n",
    "print(f\"array A has <{S}> samples of <{N}> data point for each of the <{H}> health conditions \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4d284",
   "metadata": {},
   "source": [
    "The spectra are computed with [numpy.fft.rfft](https://numpy.org/doc/stable/reference/generated/numpy.fft.rfft.html)<br>\n",
    "On the web page, you can see how to compute the size of the spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if S % 2 == 0:\n",
    "    N_spectrum = int(N/2+1)\n",
    "else:\n",
    "    N_spectrum = int((N+1)/2)\n",
    "print(f\"size of spectra: {N_spectrum}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c772e8b",
   "metadata": {},
   "source": [
    "Now you must define and dimension 3 ndarrays of `floats` to store the spectra of the 3 temporal data arrays.<br>\n",
    "For the dimensions, you must use `H`, `S` and `N_spectrum`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e150c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_spectrum = ..........\n",
    "B_spectrum = ..........\n",
    "C_spectrum = .........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe825877",
   "metadata": {},
   "source": [
    "Now you can compute the normalized spectra with the `np.fft.rfft` function and fill in the 3 arrays (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fab370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import rfft\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7b1a3",
   "metadata": {},
   "source": [
    "## 5.2 $-$ Prepare the data set for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79342509",
   "metadata": {},
   "source": [
    "As we saw previously, we can keep only the first 400 spectral points in each sample, so you must define `x_full` and `y_full` with the appropriate dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d04a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_spectrum = 400\n",
    "\n",
    "x_full = ..............\n",
    "y_full = .............."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1bf7a8",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9edf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2db07",
   "metadata": {},
   "source": [
    "Then you can fill `x_full` and `y_full` appropriately (see **4.2 −\n",
    "Prepare the labeled data for the training** to get some help):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b386e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "854fda3e",
   "metadata": {},
   "source": [
    "Let's have a quick look on the spectrum of rank 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(x_full[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a32523",
   "metadata": {},
   "source": [
    "## 5.3 Split the full dataset into train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a72234",
   "metadata": {},
   "source": [
    "see **4.3 Split the full dataset into train and test datasets** to get some help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e89a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, lab_train, lab_test = ................    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd717e",
   "metadata": {},
   "source": [
    "## 5.4 Transform labels to *one-hot* format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25761d87",
   "metadata": {},
   "source": [
    "see **4.4 Transform labels to one-hot format** to get some help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28709a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# 'one-hot' encoding' des labels :\n",
    "y_train = ....................\n",
    "y_test  = ...................."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed186cec",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75114ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, lab_train.shape, lab_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456aa406",
   "metadata": {},
   "source": [
    "## 5.5 $-$ Build the Neural Network for the spectral dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2eba39",
   "metadata": {},
   "source": [
    "Define `modelS`, the NN for the training with the spectral dataset (same as previoulsy, except the dimension of the input layer...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# set the seed of the random generators used by tensorflow:\n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Build the neural network layer by layer (note the new name: 'modelS'):\n",
    "modelS = Sequential()\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd856c3",
   "metadata": {},
   "source": [
    "We save the structure and the weights of the initial state of `modelS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check wether the 'model' directory exist (create it if needed):\n",
    "if not os.path.exists(\"models\"): os.mkdir(\"models\")\n",
    "\n",
    "# define a uniq key:\n",
    "key = 'CWRU_spectral_init'\n",
    "\n",
    "# define the path where to store the network data:\n",
    "path = os.path.join('models', key)\n",
    "\n",
    "# savue the structure and the weights of the current neural network:\n",
    "modelS.save(path)\n",
    "\n",
    "# display the tree beginning at f'./models/{key}':\n",
    "tree = scan_dir(f\"./models/{key}\")\n",
    "print(f'\\nFiles written:\\n{tree}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f49fc",
   "metadata": {},
   "source": [
    "Train the `modelS` network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ba0f4",
   "metadata": {},
   "source": [
    "You can try the earlyStopping an `val_loss` with patience of 1,2,3... to get the best scores in the confusion matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d988bf78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c92d7226",
   "metadata": {},
   "source": [
    "Now we compute the trained network predictions for the test datatset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b42a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f84ad29b",
   "metadata": {},
   "source": [
    "And we can plot the **confusion matrix** to  see if the networkd is well trained or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd72b58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
